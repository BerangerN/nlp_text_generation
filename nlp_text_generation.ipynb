{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from string import punctuation\n",
    "punctuation = punctuation[0:5]+punctuation[7:]\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "discours = pd.read_csv('discours_macron.csv')\n",
    "# discours[discours['monologue'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(texts):\n",
    "    texts = texts.apply(lambda x: x.translate(str.maketrans(punctuation, ' '*len(punctuation))))\n",
    "    texts = texts.apply(lambda x: tokenizer.tokenize(x))\n",
    "    return texts\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    array_sentence = sentence.split(\" \")\n",
    "    return array_sentence\n",
    "\n",
    "def create_word_index_unigram(texts):\n",
    "    word_to_index = {\n",
    "    \"START\":0,\n",
    "    \"END\":1\n",
    "    }\n",
    "    index_to_word = ['START', 'END']\n",
    "\n",
    "    for speech in texts:\n",
    "        for word in speech:\n",
    "            if word not in index_to_word:\n",
    "                word_to_index[word] = len(index_to_word)\n",
    "                index_to_word.append(word)\n",
    "                \n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "def create_unigram_matrix(word_to_index, index_to_word, texts):\n",
    "    V = len(index_to_word)\n",
    "    matrix = np.zeros((V, V))\n",
    "\n",
    "    for sentence in texts:\n",
    "        for i in range(len(sentence)):\n",
    "            if i == 0:\n",
    "                matrix[0, word_to_index[sentence[i]]] += 1\n",
    "\n",
    "            else: \n",
    "                matrix[word_to_index[sentence[i-1]], word_to_index[sentence[i]]] += 1\n",
    "\n",
    "        if i == (len(sentence)-1):\n",
    "            matrix[word_to_index[sentence[i]], 1] += 1\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Process texts\n",
    "texts = clean_text(discours[discours['monologue'] == True].reset_index().text)\n",
    "\n",
    "nb_processed_texts = 100\n",
    "\n",
    "#Get word to index and index to word for unigrams\n",
    "word_to_index_unigram, index_to_word_unigram = create_word_index_unigram(texts[:nb_processed_texts])\n",
    "\n",
    "#Get unigram matrix\n",
    "unigram_matrix = create_unigram_matrix(word_to_index_unigram,  index_to_word_unigram, texts[:nb_processed_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [Comme, chaque, 14, Juillet, nous, nous, rassemblons, aujourd'hui, autour, de, nos, armées, et, ...\n",
       "1      [Françaises, Français, Mes, chers, compatriotes, Je, veux, ce, soir, vous, parler, des, jours, q...\n",
       "2      [Merci, beaucoup, Madame, la, Chancelière, chère, Angela, MERKEL, En, effet, nous, avons, eu, l'...\n",
       "3      [Merci, beaucoup, Madame, la, Chancelière, chère, Angela, MERKEL, En, effet, nous, avons, eu, l'...\n",
       "4      [Un, an, après, le, lancement, à, Paris, de, l'Appel, de, Christchurch, la, France, et, la, Nouv...\n",
       "                                                      ...                                                 \n",
       "610    [Mesdames, Messieurs, les, Parlementaires, Mesdames, Messieurs, les, Présidents, les, Présidente...\n",
       "611    [Bonjour, mes, chers, compatriotes, Messieurs, les, Ministres, Monsieur, l'Ambassadeur, Madame, ...\n",
       "612    [Ce, qu'il, s'est, passé, aujourd'hui, à, Paris, n'a, rien, à, voir, avec, l'expression, pacifiq...\n",
       "613    [Mesdames, Messieurs, J'ai, souhaité, à, l'issue, de, ces, trois, jours, en, Argentine, pour, un...\n",
       "614    [Françaises, Français, nous, voilà, ensemble, au, rendez, vous, de, notre, pays, et, de, notre, ...\n",
       "Name: text, Length: 615, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_next_word(index_previous_ngram, matrix, show_nb_possibilities=False):\n",
    "    '''\n",
    "    index_previous_ngram : int, represent the word index in the word_to_index matrix\n",
    "    matrix : successor matrix, lines represent the n-gram, columns represent the successor\n",
    "\n",
    "    1. Check how many successors are possible\n",
    "    2. Select a random word from the possible words, to avoid repetition\n",
    "    '''\n",
    "    \n",
    "    if (matrix[index_previous_ngram]>0).sum() < 2:\n",
    "        nb_possible = 1\n",
    "    elif (matrix[index_previous_ngram]>0).sum() > 9:\n",
    "        nb_possible = 10\n",
    "    else:\n",
    "        nb_possible = (matrix[index_previous_ngram]>0).sum()\n",
    "\n",
    "    if show_nb_possibilities==True:\n",
    "        print('NUMBER POSSIBLE ', nb_possible)\n",
    "\n",
    "    top_indexes = matrix[index_previous_ngram].argsort()[-nb_possible:][::-1]\n",
    "    random_index = math.floor(random.random()*nb_possible)\n",
    "\n",
    "    index_next_word = top_indexes[random_index]\n",
    "        \n",
    "    return index_next_word, matrix[index_previous_ngram][index_next_word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "madame la fois encore des femmes dans cette salle provisoire de ce qu'on puisse aller plus fort chez vous et décisions le plan économique faire face ces valeurs partagées une réponse très heureux pour nos deux pays République a quelques unes qui dire point région est un peu à projets concrets territoires sont droits Et puis on doit tous les autres se trouve d'abord en France c'est ne fera monde du Fonds mondial forte retraite ministres Mesdames que sécurité commune cela ici prochaines semaines Je met retraites mais il eu C'est aussi nouvelle Président cher OUATTARA tard allaient devenir celle d'un "
     ]
    }
   ],
   "source": [
    "def get_sentence_unigram(previous_text, word_to_index, index_to_word,  matrix, nb_words_after=100, no_repetition=True):\n",
    "\n",
    "    used_words = split_sentence(previous_text)\n",
    "    last_word = used_words[len(used_words)-1]\n",
    "    print(previous_text, end=\" \")\n",
    "    index_last_word = word_to_index[last_word]\n",
    "    \n",
    "    for i in range(nb_words_after):        \n",
    "        index_last_word, occurence = get_next_word(index_last_word, matrix)\n",
    "        \n",
    "        while index_to_word[index_last_word] in used_words:\n",
    "            index_last_word, occurence = get_next_word(index_last_word, matrix)\n",
    "                \n",
    "        used_words.append(index_to_word[index_last_word])\n",
    "        print(index_to_word[index_last_word], end = ' ')\n",
    "        if(index_last_word==1):\n",
    "            break;\n",
    "\n",
    "get_sentence_unigram(\"madame\", word_to_index_unigram, index_to_word_unigram,unigram_matrix,nb_words_after=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Le'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word_unigram[get_next_word(word_to_index_unigram['START'], unigram_matrix)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIGRAMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_word_index_bigram(texts):\n",
    "    word_to_index = {\n",
    "    \"START START\":0,\n",
    "    \"END\":1\n",
    "    }\n",
    "    index_to_word = ['START START', 'END']\n",
    "\n",
    "    for speech in texts:\n",
    "        for i in range(len(speech)):\n",
    "            if i == 0:\n",
    "                word = 'START ' + speech[0]\n",
    "            else:\n",
    "                word = speech[i-1] + ' ' + speech[i]\n",
    "\n",
    "            if word not in index_to_word:\n",
    "                word_to_index[word] = len(index_to_word)\n",
    "                index_to_word.append(word)\n",
    "            \n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "def create_bigram_matrix(word_to_index, index_to_word, word_to_index_unigram, texts):\n",
    "    V = len(index_to_word)\n",
    "    V_unigram = len(word_to_index_unigram)\n",
    "    matrix = np.zeros((V, V_unigram))\n",
    "\n",
    "    for sentence in texts:\n",
    "        for i in range(len(sentence)):\n",
    "            if i == 0:\n",
    "                matrix[word_to_index['START START'], word_to_index_unigram[sentence[0]]] += 1            \n",
    "                matrix[word_to_index['START '+sentence[0]], word_to_index_unigram[sentence[1]]] += 1\n",
    "\n",
    "            elif i >= (len(sentence)-1):\n",
    "                matrix[word_to_index[sentence[i-1]+' '+sentence[i]], 1] += 1\n",
    "       \n",
    "            else: \n",
    "                matrix[word_to_index[sentence[i-1]+' '+sentence[i]], word_to_index_unigram[sentence[i+1]]] += 1\n",
    "\n",
    "    #matrix /= matrix.sum(axis=1, keepdims=False)\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get word to index and index to word for bigrams\n",
    "word_to_index_bigram, index_to_word_bigram = create_word_index_bigram(texts[:nb_processed_texts])\n",
    "\n",
    "#Get bigram matrix\n",
    "bigram_matrix = create_bigram_matrix(word_to_index_bigram, index_to_word_bigram, word_to_index_unigram, texts[:nb_processed_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['START START',\n",
       " 'END',\n",
       " 'START Comme',\n",
       " 'Comme chaque',\n",
       " 'chaque 14',\n",
       " '14 Juillet',\n",
       " 'Juillet nous',\n",
       " 'nous nous',\n",
       " 'nous rassemblons',\n",
       " \"rassemblons aujourd'hui\",\n",
       " \"aujourd'hui autour\",\n",
       " 'autour de',\n",
       " 'de nos',\n",
       " 'nos armées',\n",
       " 'armées et',\n",
       " 'et nous',\n",
       " 'nous faisons',\n",
       " 'faisons corps',\n",
       " 'corps avec',\n",
       " 'avec les']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word_bigram[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_bigram(previous_text, word_to_index, index_to_word, index_to_word_unigram, matrix, nb_words_after=20, no_repetition=True):\n",
    "\n",
    "    used_words = split_sentence(previous_text)\n",
    "    last_bigram = used_words[len(used_words)-2] + ' ' + used_words[len(used_words)-1]\n",
    "    \n",
    "    print(previous_text, end=\" \")\n",
    "\n",
    "    index_last_bigram = word_to_index[last_bigram]\n",
    "\n",
    "    for i in range(nb_words_after):  \n",
    "        \n",
    "        index_next_word, occurence = get_next_word(index_last_bigram, matrix)\n",
    "        \n",
    "       # while index_to_word[index_last_bigram] in used_words:\n",
    "       #     index_last_bigram, occurence = get_next_word_bigram(index_last_bigram, matrix)\n",
    "                \n",
    "        used_words.append(index_to_word_unigram[index_next_word])\n",
    "        print(index_to_word_unigram[index_next_word], end = ' ')\n",
    "        if(index_next_word==1):\n",
    "            break;\n",
    "        \n",
    "        index_last_bigram = word_to_index[used_words[len(used_words)-2] + ' ' + used_words[len(used_words)-1]]\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compatriotes\n",
      "24.0\n"
     ]
    }
   ],
   "source": [
    "idx, occurence = get_next_word(word_to_index_bigram['Mes chers'], bigram_matrix)\n",
    "print(index_to_word_unigram[idx])\n",
    "print(occurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START START Mes chers compatriotes Depuis quelques semaines Nous tirerons les conséquences concrètes pour vous c'est fait pour le pays Nos associations les travailleurs jusqu'en 2025 ils sont une famille y monter un garage tombé devant sa femme qui aura su bâtir un nouveau partenariat Je pense aux médecins aux infirmiers aux infirmières aux aides soignants aides soignantes qui ont donné leur vie particulière pour cette raison que c'est très cher et d'ailleurs beaucoup mieux dans le nouveau parlement européen qui doit tant à l'engagement pris par le ministre le Ministre ici présent aussi allant des questions à vous de la biodiversité "
     ]
    }
   ],
   "source": [
    "get_sentence_bigram(\"START START\", \n",
    "                    word_to_index_bigram, \n",
    "                    index_to_word_bigram, \n",
    "                    index_to_word_unigram, \n",
    "                    bigram_matrix,\n",
    "                   nb_words_after=100\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_word_index_trigram(texts):\n",
    "    word_to_index = {\n",
    "    \"START START START\":0,\n",
    "    \"END\":1\n",
    "    }\n",
    "    index_to_word = ['START START START', 'END']\n",
    "\n",
    "    for speech in texts:\n",
    "        for i in range(len(speech)):\n",
    "            if i == 0:\n",
    "                word = 'START START ' + speech[i]\n",
    "            elif i==1:\n",
    "                word = 'START ' + speech[i-1] + ' ' + speech[i]\n",
    "            else:\n",
    "                word = speech[i-2] + ' ' + speech[i-1] + ' ' + speech[i]\n",
    "\n",
    "            #print(word)\n",
    "            if word not in index_to_word:\n",
    "                word_to_index[word] = len(index_to_word)\n",
    "                index_to_word.append(word)\n",
    "            \n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "def create_trigram_matrix(word_to_index, index_to_word, word_to_index_unigram, texts):\n",
    "    V = len(index_to_word)\n",
    "    V_unigram = len(word_to_index_unigram)\n",
    "    matrix = np.zeros((V, V_unigram))\n",
    "\n",
    "    for sentence in texts:\n",
    "        for i in range(len(sentence)):\n",
    "            if i == 0:\n",
    "                matrix[word_to_index['START START START'], word_to_index_unigram[sentence[0]]] += 1 \n",
    "                matrix[word_to_index['START START '+sentence[0]], word_to_index_unigram[sentence[i+1]]] += 1 \n",
    "\n",
    "            elif i ==1:\n",
    "                matrix[word_to_index['START '+sentence[i-1] + ' ' +sentence[i]], word_to_index_unigram[sentence[i+1]]] += 1\n",
    "\n",
    "            elif i >= (len(sentence)-1):\n",
    "                matrix[word_to_index[sentence[i-2] +' '+ sentence[i-1]+' '+sentence[i]], 1] += 1\n",
    "       \n",
    "            else: \n",
    "                matrix[word_to_index[sentence[i-2]+' '+sentence[i-1]+' '+sentence[i]], word_to_index_unigram[sentence[i+1]]] += 1\n",
    "\n",
    "    #matrix /= matrix.sum(axis=1, keepdims=False)\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index_trigram, index_to_word_trigram = create_word_index_trigram(texts[:nb_processed_texts])\n",
    "\n",
    "trigram_matrix = create_trigram_matrix(word_to_index_trigram, index_to_word_trigram, word_to_index_unigram, texts[:nb_processed_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      1],\n",
       "       [     1, 243787],\n",
       "       [     2,  15768],\n",
       "       [     3,   4377],\n",
       "       [     4,   1981],\n",
       "       [     5,    991],\n",
       "       [     6,    616],\n",
       "       [     7,    385],\n",
       "       [     8,    273],\n",
       "       [     9,    212],\n",
       "       [    10,    151],\n",
       "       [    11,    122],\n",
       "       [    12,     85],\n",
       "       [    13,     62],\n",
       "       [    14,     61],\n",
       "       [    15,     57],\n",
       "       [    16,     43],\n",
       "       [    17,     35],\n",
       "       [    18,     29],\n",
       "       [    19,     26],\n",
       "       [    20,      8],\n",
       "       [    21,     15],\n",
       "       [    22,     19],\n",
       "       [    23,     17],\n",
       "       [    24,      7],\n",
       "       [    25,     13],\n",
       "       [    26,     12],\n",
       "       [    27,     10],\n",
       "       [    28,     10],\n",
       "       [    29,      1],\n",
       "       [    30,      4],\n",
       "       [    31,      6],\n",
       "       [    32,      2],\n",
       "       [    33,      4],\n",
       "       [    34,      4],\n",
       "       [    35,      1],\n",
       "       [    36,      1],\n",
       "       [    37,      2],\n",
       "       [    38,      5],\n",
       "       [    39,      3],\n",
       "       [    40,      2],\n",
       "       [    41,      3],\n",
       "       [    42,      4],\n",
       "       [    43,      1],\n",
       "       [    44,      1],\n",
       "       [    45,      2],\n",
       "       [    47,      1],\n",
       "       [    49,      1],\n",
       "       [    51,      2],\n",
       "       [    53,      3],\n",
       "       [    56,      3],\n",
       "       [    58,      3],\n",
       "       [    59,      1],\n",
       "       [    61,      1],\n",
       "       [    63,      1],\n",
       "       [    64,      1],\n",
       "       [    65,      2],\n",
       "       [    66,      1],\n",
       "       [    74,      1],\n",
       "       [    80,      1],\n",
       "       [    82,      1],\n",
       "       [    89,      1],\n",
       "       [    93,      1],\n",
       "       [   101,      1],\n",
       "       [   102,      1],\n",
       "       [   130,      1]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique((trigram_matrix!=0).sum(1), return_counts=True)\n",
    "\n",
    "np.asarray((unique, counts)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.54399863322526"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Proportion of non random selection\n",
    "(((trigram_matrix!=0).sum(1))==1).sum()*100/trigram_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER POSSIBLE  6\n",
      "nous 1.0\n"
     ]
    }
   ],
   "source": [
    "idx, occ = get_next_word(word_to_index_trigram['on va devoir'], trigram_matrix,show_nb_possibilities=True)\n",
    "print(index_to_word_unigram[idx], occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentence_trigram(previous_text, word_to_index, index_to_word, index_to_word_unigram, matrix, nb_words_after=20, no_repetition=True):\n",
    "\n",
    "    used_words = split_sentence(previous_text)\n",
    "    last_trigram = used_words[len(used_words)-3] + ' ' + used_words[len(used_words)-2] + ' ' + used_words[len(used_words)-1]\n",
    "    \n",
    "    print(previous_text, end=\" \")\n",
    "\n",
    "    index_last_trigram = word_to_index[last_trigram]\n",
    "\n",
    "    for i in range(nb_words_after):  \n",
    "        \n",
    "        index_next_word, occurence = get_next_word(index_last_trigram, matrix)\n",
    "\n",
    "       # while index_to_word[index_last_trigram] in used_words:\n",
    "       #     index_last_trigram, occurence = get_next_word_bigram(index_last_trigram, matrix)\n",
    "\n",
    "        used_words.append(index_to_word_unigram[index_next_word])\n",
    "        print(index_to_word_unigram[index_next_word], end = ' ')\n",
    "        if(index_next_word==1):\n",
    "            break;\n",
    "        \n",
    "        index_last_trigram = word_to_index[used_words[len(used_words)-3] + ' ' + used_words[len(used_words)-2] + ' ' + used_words[len(used_words)-1]]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START START START Françaises Français mes chers compatriotes La réserve sanitaire d'abord avec plus de 40 000 litres jour à 500 000 litres jour à 500 000 litres jour Je veux remercier le ministre de la Culture s'est fortement exprimé sur ce sujet de manière tout à fait le droit d'être moi même le lanceur d'alerte sur ce sujet pour ce qui me concerne je tâcherai de porter en Europe notre voix afin d'avoir plus d'unité et de diversité Eco avait cette formule magnifique en disant la langue de la République il déborde Valeurs actuelles par leur droite ou par leur gauche ce que … GUILLAUME ERNER Ce n'est pas simplement un décret c'est une mobilisation nationale Pour ça j'ai besoin des entreprises pour là aussi faire confiance à ces publics Il y a ensuite l'hospitalité qui fait que vous voir déambuler dans votre ville c'est avoir la conviction que nous poursuivrons notre travail longuement discuté ces dernières semaines avec le débat sur l'immigration qu'il avait souhaité au sortir du grand débat national après la crise des migrants Mais l'Italie le pays le plus attractif en termes d'investissements que j'avais pris suite justement à la reconnaissance du droit de recours et acceptant dans le continent même qui avait pensé la souveraineté du peuple et la correspondance entre celui ci et l'espace juridique la possibilité de produire qui se sont attachés à avoir une carrière qui passe du public au privé d'une entreprise à l'autre Vous pouvez avoir la fierté d'aider à les former Quand on a plus de raison d'avoir peur du seuil fatidique en se disant que c'est un secteur qui est en France aux essais cliniques sur l'hydroxychloroquine azithromycine Donc ce qui est aujourd'hui en train de vivre des jours difficiles Nous ressentons tous en ce moment est ce qu'on dit le plus important pour eux ce qui n'avait pas de système du chômage partiel vient d'annoncer qu'ils vont mettre en place pour accompagner ces changements qu'il s'agisse d'ailleurs des artisans des industriels et donc qu'il s'agisse du sport de la culture et ça ça a été un vrai changement qui nécessite une réflexion stratégique au sein de l'Union européenne pour permettre un vrai soutien aux pays les plus pauvres dans nos banlieues ou dans d'autres quartiers de la République C'est ainsi qu'il faut mener ce combat Et nous devons avant tout sécuriser notre capacité à dépasser les clivages politiques On "
     ]
    }
   ],
   "source": [
    "get_sentence_trigram(\"START START START\", \n",
    "                    word_to_index_trigram, \n",
    "                    index_to_word_trigram, \n",
    "                    index_to_word_unigram, \n",
    "                    trigram_matrix,\n",
    "                   nb_words_after=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
